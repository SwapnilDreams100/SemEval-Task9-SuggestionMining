{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A notebook solving the SemEval 2019 suggestion mining challenge."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "\n",
    "NLP has had its ImageNet moment in the advent of Transfer learning frameworks like BERT and embeddings like GLOVE, Universal Sentences Encoding, Fasttext vectors etc. \n",
    "The present problem of suggestion mining will be solved using the following approaches demostrating these techniques: \n",
    "\n",
    "0. Baseline using Fasttext classification\n",
    "1. Using GLOVE for traditional embeddings.\n",
    "GLOVE vectors are trained using CBOW(Continuous Bag of Words) or ngram, take only context in one direction.\n",
    "2. Pre-Trained Word embeddings using BERT for classification.\n",
    "BERT is trained Bidirectionally using a Transformer architechture, thus utilizing both contexts.\n",
    "3. Domain FineTuning of BERT embeddings for classification.\n",
    "Domain Finetuning helps to update the embeddings according to the domain, improving classification predictions.\n",
    "4. Classification using Gradient boosted trees and Neural Nets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import re\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gc\n",
    "\n",
    "from gensim.models import KeyedVectors\n",
    "import time, os\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import fasttext as ft\n",
    "\n",
    "import lightgbm as lgb\n",
    "from bert_serving.client import BertClient\n",
    "\n",
    "from keras.layers import Dense, Input, Bidirectional, Activation, Conv1D, GRU, Lambda, LSTM\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers import GlobalAveragePooling1D,MaxPooling1D,GlobalMaxPooling1D, concatenate, SpatialDropout1D\n",
    "from keras.layers import Dropout,Embedding,Add,Flatten\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.callbacks import Callback,EarlyStopping,ModelCheckpoint\n",
    "from keras.models import Model, load_model\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers.merge import concatenate\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SPELL CHECKER - disabled due to lack of improvement\n",
    "\n",
    "# from symspellpy.symspellpy import SymSpell\n",
    "\n",
    "# max_edit_distance_dictionary = 2\n",
    "# prefix_length = 7\n",
    "# sym_spell = SymSpell(max_edit_distance_dictionary, prefix_length)\n",
    "\n",
    "# dictionary_path = \"./spell_check_dictionary.txt\"\n",
    "# term_index = 0\n",
    "# count_index = 1\n",
    "\n",
    "# if not sym_spell.load_dictionary(dictionary_path, term_index, count_index):\n",
    "#     print('no spell dict')\n",
    "# print('spell check model loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Paths\n",
    "train_data='./data/train.csv'\n",
    "val_data='./data/val_lab.csv'\n",
    "test_data='./data/test.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the data into pandas dataframe.  \n",
    "train_df = pd.read_csv(train_data, names = [\"id\", \"sentence\", \"label\"], encoding = \"ISO-8859-1\")\n",
    "val_df = pd.read_csv(val_data, encoding = \"ISO-8859-1\" )\n",
    "test_df = pd.read_csv(test_data, names = [\"id\", \"sentence\", \"label\"], encoding = \"ISO-8859-1\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    \"Please enable removing language code from the...\n",
       "1    \"Note: in your .csproj file, there is a Suppor...\n",
       "2    \"Wich means the new version not fully replaced...\n",
       "3    \"Some of my users will still receive the old x...\n",
       "4    \"The store randomly gives the old xap or the n...\n",
       "Name: sentence, dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df['sentence'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    6415\n",
       "1    2085\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df['label'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Class Imbalance exists. Can be rectified by Oversampling - Using SMOTE, in order to randomly sample k nearest neighbors. \n",
    "First lets clean the data to make it easier to get consistent embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Complete Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## A spelling correction function, maybe used in the future, could not yield increase in accuracy\n",
    "\n",
    "# def spell_check(text):\n",
    "#     try:\n",
    "#         max_edit_distance_lookup = 2\n",
    "#         suggestions = sym_spell.lookup_compound(text, max_edit_distance_lookup)\n",
    "\n",
    "#         for suggestion in suggestions:\n",
    "#             return(suggestion.term)\n",
    "        \n",
    "#     except Exception as e:\n",
    "#         return\n",
    "\n",
    "## Replacing key abbreviations/slang and bigrams\n",
    "\n",
    "def rep(sent):\n",
    "    sent = sent.replace(' u ', \" you \").replace(' im ', ' i am ') \\\n",
    "    .replace(' r ', \" are \").replace(' u ', ' you ').replace('plz','please').replace(' pc ',' computer ') \\\n",
    "    .replace('i\\'m','i am').replace(\"'s\",'is').replace(\"'t\",'not').replace(\"'d\",'would').replace(' e g ',' example') \\\n",
    "    .replace(\"doesn t\",'does not').replace(\"e.g\",'example') \\\n",
    "    .replace(\"i.e.\",\"that is\").replace('wich','which')\n",
    "    \n",
    "#     sent = spell_check(sent)\n",
    "    sent = \" \".join(sent.split())\n",
    "    return sent\n",
    "\n",
    "## Main preprocessing function\n",
    "def preprocess(x):\n",
    "    \n",
    "    # Preprocess URLS\n",
    "    x.loc[:,\"sentence\"] = x.sentence.apply(lambda x : re.sub(r'http\\S+', '', x) )\n",
    "    \n",
    "    # Lower Case all string to help increase uniformity in embedding creation\n",
    "    x.loc[:,\"sentence\"] = x.sentence.apply(lambda x : str.lower(x))\n",
    "\n",
    "    #Remove numbers, special characters\n",
    "    x.loc[:,\"sentence\"] = x.sentence.apply(lambda x : (re.sub(r'([^a-zA-Z ]+?)', '', x)))\n",
    "\n",
    "    # Replace abbs/other modeifications\n",
    "    x.loc[:,\"sentence\"] = x.sentence.apply(lambda x : rep(x))\n",
    "    \n",
    "    # Remove empty strings, containing only urls etc.\n",
    "    x = x[x.sentence != '']\n",
    "    x = x[x.sentence != ' ']\n",
    "    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess Training and Validation Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = preprocess(train_df)\n",
    "val_df = preprocess(val_df)\n",
    "test_df = preprocess(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    please enable removing language code from the ...\n",
       "1    note in your csproj file there is a supportedc...\n",
       "2    which means the new version not fully replaced...\n",
       "3    some of my users will still receive the old xa...\n",
       "4    the store randomly gives the old xap or the ne...\n",
       "Name: sentence, dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df['sentence'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    6381\n",
       "1    2085\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 0 label counts decreased due to preprocessing\n",
    "train_df['label'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) Using BERT embeddings for classification\n",
    "In this section the first approach; of using bert embeddings(using bert-as-a-service server) is covered.\n",
    "BERT encoding is performed on the Training, Validation and the Test Set.\n",
    "Then the training data is split to train the LightGBM model, which is a fast gradient boosting framework.\n",
    "Its hyperparams are tuned by using Bayesian tuning using skopt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization + Encoding with BERT Embedding - Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "bc = BertClient()\n",
    "X_train=[]\n",
    "from tqdm import tqdm\n",
    "for i in tqdm(list(train_df['sentence'])): \n",
    "    print([i])\n",
    "    X_train.append(bc.encode([i]))\n",
    "y_train = list(train_df['label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilies for storing and retrieving the embeddings(Since they take a while to be created)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('bert_array.pkl', 'wb') as f:\n",
    "    pickle.dump(X_train, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('bert_array.pkl', 'rb') as f:\n",
    "    X_train = pickle.load( f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code for flattening the embedding array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_n = []\n",
    "for sublist in X_train:\n",
    "    for item in sublist:\n",
    "        X_train_n.append(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using SMOTE for class imbalance\n",
    "1. Simply OverSampling: has disadvantages in information duplication and overfitting\n",
    "2. Simply UnderSampling: has disadvantages in  information loss.\n",
    "3. Hence OverSampling using smote based on the data distribution is most useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "sm = SMOTE(random_state=12, ratio = 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_bal, y_train_bal = sm.fit_sample(X_train_n, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the number of 0 and 1 label samples have been equalized by number of Y observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12762"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_train_bal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is no class imbalance in the Validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    296\n",
       "0    296\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# No Imbalance here\n",
    "val_df['label'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization + Encoding with BERT Embedding - Validation Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "bc = BertClient()\n",
    "X_val=[]\n",
    "from tqdm import tqdm\n",
    "for i in tqdm(list(val_df['sentence'])): \n",
    "    print([i])\n",
    "    X_val.append(bc.encode([i]))\n",
    "y_val = list(val_df['label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilies for saving and retrieving validation BERT embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('bert_array_val.pkl', 'wb') as f:\n",
    "    pickle.dump(X_val, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('bert_array_val.pkl', 'rb') as f:\n",
    "    X_val = pickle.load( f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Flattening the validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val_n = []\n",
    "for sublist in X_val:\n",
    "    for item in sublist:\n",
    "        X_val_n.append(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization + Encoding with BERT Embedding - Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "bc = BertClient()\n",
    "X_test=[]\n",
    "from tqdm import tqdm\n",
    "for i in tqdm(list(test_df['sentence'])): \n",
    "    print([i])\n",
    "    X_test.append(bc.encode([i]))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilies for saving and retrieving validation BERT embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('bert_array_test.pkl', 'wb') as f:\n",
    "#     pickle.dump(X_test, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('bert_array_test.pkl', 'rb') as f:\n",
    "    X_test = pickle.load( f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Flattening the validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_n = []\n",
    "for sublist in X_test:\n",
    "    for item in sublist:\n",
    "        X_test_n.append(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a lightGBM gradient boosting algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[100]\tvalid_0's binary_logloss: 0.185929\tvalid_0's f1_score: 0.93007\n",
      "[200]\tvalid_0's binary_logloss: 0.194392\tvalid_0's f1_score: 0.932816\n",
      "Early stopping, best iteration is:\n",
      "[121]\tvalid_0's binary_logloss: 0.182664\tvalid_0's f1_score: 0.930793\n",
      "training...\n"
     ]
    }
   ],
   "source": [
    "# Method for evaluating fi score for lightgbm\n",
    "def lgb_f1_score(y_preds, data):\n",
    "    y_train = data.get_label()\n",
    "    y_preds = (y_preds >= 0.5).astype(int)\n",
    "    return 'f1_score', f1_score(y_train, y_preds), True\n",
    "\n",
    "params = {\n",
    "          'objective':'binary', \n",
    "          \"boosting\": \"gbdt\",  \n",
    "          'learning_rate': 0.1, \n",
    "          'max_depth': -1, \n",
    "          \"feature_fraction\": 0.8, \n",
    "          \"bagging_freq\": 1, \n",
    "          \"bagging_fraction\": 0.8 , \n",
    "          \"bagging_seed\": 11,\n",
    "          \"lambda_l1\": 0.1, \n",
    "          'num_leaves': 60, \n",
    "          'min_data_in_leaf': 60, \n",
    "          \"verbosity\": -1, \n",
    "          \"random_state\": 3\n",
    "         }\n",
    "\n",
    "# Shuffling Data\n",
    "shuffle_indices = np.random.permutation(np.arange(len(y_train_bal)))\n",
    "X_train_bal=np.array(X_train_bal)\n",
    "y_train_bal=np.array(y_train_bal)\n",
    "    \n",
    "X_train_bal_shuffled = X_train_bal[shuffle_indices]\n",
    "y_train_bal_shuffled = y_train_bal[shuffle_indices]\n",
    "\n",
    "# Splitting training data into Training and Validation\n",
    "val_idx = max(1, int(len(y_train_bal_shuffled) * 0.2))\n",
    "train_data_split, val_data_split = X_train_bal_shuffled[:-val_idx], X_train_bal_shuffled[-val_idx:]\n",
    "labels_train, labels_val = y_train_bal_shuffled[:-val_idx], y_train_bal_shuffled[-val_idx:]\n",
    "\n",
    "# Training model\n",
    "model = lgb.train(params, lgb.Dataset(train_data_split, labels_train), \\\n",
    "                  2500, lgb.Dataset(val_data_split, labels_val), \\\n",
    "                  feval=lgb_f1_score ,verbose_eval=100, early_stopping_rounds=100)\n",
    "model.save_model('lgb.model')\n",
    "\n",
    "print('training...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7654320987654321\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          NS       0.75      0.82      0.78       296\n",
      "           S       0.80      0.73      0.77       296\n",
      "\n",
      "   micro avg       0.78      0.78      0.78       592\n",
      "   macro avg       0.78      0.78      0.77       592\n",
      "weighted avg       0.78      0.78      0.77       592\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lgb_val_pred = model.predict(X_val_n)\n",
    "\n",
    "lgb_val_pred = (lgb_val_pred >= 0.5).astype(int)\n",
    "print(f1_score(y_val, lgb_val_pred))\n",
    "print(classification_report(y_val, lgb_val_pred, target_names=['NS','S']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb_test_pred = model.predict(X_test_n)\n",
    "lgb_test_pred = (lgb_test_pred >= 0.5).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) Finetuning BERT Transformer to domain\n",
    "In this approach, I finetuned the BERT pre-trained Uncased 768D embeddings, to the training data. \n",
    "Using the BERT script provided in the bert repository `run_classifier.py`, the pretrained model was finetuned to the given domain data. \n",
    "This process is called transfer learning, and is relatively inexpensive to perform, in comparison to training the full neural network from scratch. \n",
    "The bert validation prediction and test are uploaded alongside the notebook "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the evaluation of the validation predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_val_preds = pd.read_csv('C:/Users/admin/InternshipTask/NLP Task/bert_dev_results.tsv',sep='\\t', names=['label0','label1'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BERT model outputs the probabilies of each class, so we need to create the predictions by taking the label with maximum probabililty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "C:\\Users\\admin\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\ipykernel_launcher.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "for ind,row in bert_val_preds.iterrows():\n",
    "    if row['label0']>row['label1']:\n",
    "        bert_val_preds['label_pred'][ind] = 0\n",
    "    else:\n",
    "        bert_val_preds['label_pred'][ind] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluating the performance. \n",
    "BERT with finetuning gives this highest f1 score out of all approaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8694214876033058\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          NS       0.88      0.84      0.86       296\n",
      "           S       0.85      0.89      0.87       296\n",
      "\n",
      "   micro avg       0.87      0.87      0.87       592\n",
      "   macro avg       0.87      0.87      0.87       592\n",
      "weighted avg       0.87      0.87      0.87       592\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bert_fn_val_pred = list(bert_val_preds['label_pred'])\n",
    "print(f1_score(y_val, bert_fn_val_pred))\n",
    "print(classification_report(y_val, bert_fn_val_pred, target_names=['NS','S']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get BERT model test predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_test_preds = pd.read_csv('C:/Users/admin/InternshipTask/NLP Task/bert_test_results.tsv',sep='\\t', names=['label0','label1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  after removing the cwd from sys.path.\n",
      "C:\\Users\\admin\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\ipykernel_launcher.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "bert_test_preds['label_pred']=0\n",
    "for ind,row in bert_test_preds.iterrows():\n",
    "    if row['label0']>row['label1']:\n",
    "        bert_test_preds['label_pred'][ind] = 0\n",
    "    else:\n",
    "        bert_test_preds['label_pred'][ind] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) GLOVE Approach\n",
    "Here the GLOVE 100D embeddings were used to train a neural network model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating training and validation set for Neural Network approach\n",
    "\n",
    "Steps: \n",
    "1. Tokenizing the sentence string\n",
    "2. Use GLOVE embedding to find word vectors\n",
    "Any other embedding could also be used such as fastText, Facebooks InferSent, Google's Universal encoder etc. \n",
    "GLOve was chosen due to its low weight and ease of use.\n",
    "Using different embedding mechanisms can have significant effect on the outcome.\n",
    "3. Train a neural net composed of GRUs, CNNs, and dropout (selected post permutation) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the Stanford GloVe model\n",
    "filename = 'glove.6B.100d.txt.word2vec'\n",
    "glove_model = KeyedVectors.load_word2vec_format(filename, binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_train_set(df, model, split=0.2, max_seq_len=0):\n",
    "    \n",
    "    total_array = list(df['sentence'])\n",
    "    s1 = list(df['label'])\n",
    "    \n",
    "    total_array1 = []\n",
    "    maxlen =0\n",
    "    \n",
    "    for sent in total_array:\n",
    "        arrayin=[]\n",
    "        count = 0\n",
    "        for word in sent.split(' '):\n",
    "            word = word.replace(' ','')\n",
    "            try:\n",
    "                a = model[word]\n",
    "                arrayin.append(a)\n",
    "                count+=1\n",
    "            except Exception as e:\n",
    "                #print(e) -- for out of dictionary words\n",
    "                continue\n",
    "        if count>maxlen:\n",
    "            maxlen = count\n",
    "        total_array1.append(arrayin)\n",
    "    \n",
    "    # check if train set or val set.\n",
    "    if max_seq_len == 0:\n",
    "        max_seq_len=maxlen\n",
    "    \n",
    "    total_array = total_array1\n",
    "    \n",
    "    # Padding the string sequences\n",
    "    train_padded_data = pad_sequences(total_array, maxlen=max_seq_len)\n",
    "    \n",
    "    train_data=train_padded_data\n",
    "    train_labels= s1\n",
    "    \n",
    "    np.random.seed(42)\n",
    "    shuffle_indices = np.random.permutation(np.arange(len(train_labels)))\n",
    "    \n",
    "    train_data=np.array(train_data)\n",
    "    train_labels=np.array(train_labels)\n",
    "    \n",
    "    train_data_shuffled = train_data[shuffle_indices]\n",
    "    train_labels_shuffled = train_labels[shuffle_indices]\n",
    "\n",
    "    val_idx = max(1, int(len(train_labels_shuffled) * split))\n",
    "    \n",
    "    train_data_split, val_data_split = train_data_shuffled[:-val_idx], train_data_shuffled[-val_idx:]\n",
    "    labels_train, labels_val = train_labels_shuffled[:-val_idx], train_labels_shuffled[-val_idx:]\n",
    "    \n",
    "    return train_data_split, labels_train, val_data_split, labels_val, max_seq_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model_GRU_CNN:\n",
    "    \n",
    "    def __init__(self, embedding_dim, model, lr = 0.0, lr_d = 0.0, units = 0, dr = 0.0):\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.lr = lr\n",
    "        self.lr_d = lr_d\n",
    "        self.units = units\n",
    "        self.dr = dr\n",
    "        self.model = model\n",
    "        \n",
    "    def train_model(self,):\n",
    "        \"\"\"\n",
    "            Steps:\n",
    "                1. Create training and val data\n",
    "                3. Pass the  dense layer vectors to sigmoid output layer\n",
    "                4. Use loss to train weights\n",
    "        \"\"\"\n",
    "        global train_df, val_df \n",
    "        train_data, train_labels, val_data, val_labels, self.max_len = create_train_set(train_df, self.model, max_seq_len=0)\n",
    "        \n",
    "        sequence_input = Input(shape=(self.max_len,self.embedding_dim,))\n",
    "        x1 = SpatialDropout1D(self.dr)(sequence_input)\n",
    "\n",
    "        x = Bidirectional(GRU(self.units, return_sequences = True))(x1)\n",
    "        x = Conv1D((self.units//2), kernel_size = 2, padding = \"valid\", kernel_initializer = \"he_uniform\")(x)\n",
    "\n",
    "        y = Bidirectional(LSTM(self.units, return_sequences = True))(x1)\n",
    "        y = Conv1D((self.units//2), kernel_size = 2, padding = \"valid\", kernel_initializer = \"he_uniform\")(y)\n",
    "\n",
    "        avg_pool1 = GlobalAveragePooling1D()(x)\n",
    "        max_pool1 = GlobalMaxPooling1D()(x)\n",
    "\n",
    "        avg_pool2 = GlobalAveragePooling1D()(y)\n",
    "        max_pool2 = GlobalMaxPooling1D()(y)\n",
    "\n",
    "        x = concatenate([avg_pool1, max_pool1, avg_pool2, max_pool2])\n",
    "\n",
    "        x = Dense(1, activation = \"sigmoid\")(x)\n",
    "        \n",
    "        model = Model(inputs = sequence_input, outputs = x)\n",
    "        model.compile(loss = \"binary_crossentropy\", optimizer = Adam(lr = self.lr, decay = self.lr_d), metrics = [\"accuracy\"])\n",
    "        \n",
    "        early_stopping = EarlyStopping(monitor='val_loss', patience=3)\n",
    "\n",
    "        timestamp = 'lstm_%d_%d_%.2f_%.2f' % (self.embedding_dim ,self.lr, self.lr_d, self.dr)\n",
    "\n",
    "        checkpoint_dir = './checkpoints/' + str(int(time.time())) + '/'\n",
    "\n",
    "        if not os.path.exists(checkpoint_dir):\n",
    "            os.makedirs(checkpoint_dir)\n",
    "\n",
    "        best_model_path = checkpoint_dir + timestamp + '.h5'\n",
    "\n",
    "        model_checkpoint = ModelCheckpoint(best_model_path, save_best_only=True, save_weights_only=False)\n",
    "\n",
    "        self.history = model.fit(train_data, train_labels, validation_data = (val_data, val_labels), \n",
    "                            verbose = 1,epochs=10, batch_size=128, shuffle=True,\n",
    "                  callbacks=[early_stopping,model_checkpoint])\n",
    "\n",
    "        return best_model_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6773 samples, validate on 1693 samples\n",
      "Epoch 1/10\n",
      "6773/6773 [==============================] - ETA: 11:02 - loss: 0.6272 - acc: 0.77 - ETA: 6:02 - loss: 0.6160 - acc: 0.7578 - ETA: 4:23 - loss: 0.5791 - acc: 0.778 - ETA: 3:31 - loss: 0.5893 - acc: 0.769 - ETA: 2:59 - loss: 0.5947 - acc: 0.764 - ETA: 2:38 - loss: 0.5990 - acc: 0.760 - ETA: 2:23 - loss: 0.5933 - acc: 0.762 - ETA: 2:10 - loss: 0.5941 - acc: 0.761 - ETA: 2:01 - loss: 0.5916 - acc: 0.760 - ETA: 1:52 - loss: 0.5957 - acc: 0.754 - ETA: 1:45 - loss: 0.5940 - acc: 0.752 - ETA: 1:39 - loss: 0.5914 - acc: 0.753 - ETA: 1:34 - loss: 0.5885 - acc: 0.754 - ETA: 1:30 - loss: 0.5853 - acc: 0.757 - ETA: 1:26 - loss: 0.5828 - acc: 0.759 - ETA: 1:22 - loss: 0.5768 - acc: 0.765 - ETA: 1:18 - loss: 0.5803 - acc: 0.760 - ETA: 1:15 - loss: 0.5796 - acc: 0.758 - ETA: 1:12 - loss: 0.5791 - acc: 0.757 - ETA: 1:09 - loss: 0.5798 - acc: 0.755 - ETA: 1:06 - loss: 0.5775 - acc: 0.756 - ETA: 1:03 - loss: 0.5759 - acc: 0.755 - ETA: 1:00 - loss: 0.5754 - acc: 0.754 - ETA: 58s - loss: 0.5756 - acc: 0.753 - ETA: 55s - loss: 0.5724 - acc: 0.75 - ETA: 53s - loss: 0.5708 - acc: 0.75 - ETA: 50s - loss: 0.5717 - acc: 0.75 - ETA: 48s - loss: 0.5682 - acc: 0.75 - ETA: 46s - loss: 0.5687 - acc: 0.75 - ETA: 43s - loss: 0.5683 - acc: 0.75 - ETA: 41s - loss: 0.5671 - acc: 0.75 - ETA: 39s - loss: 0.5664 - acc: 0.75 - ETA: 37s - loss: 0.5643 - acc: 0.75 - ETA: 35s - loss: 0.5623 - acc: 0.75 - ETA: 33s - loss: 0.5604 - acc: 0.75 - ETA: 31s - loss: 0.5585 - acc: 0.75 - ETA: 29s - loss: 0.5586 - acc: 0.75 - ETA: 27s - loss: 0.5601 - acc: 0.75 - ETA: 25s - loss: 0.5572 - acc: 0.75 - ETA: 23s - loss: 0.5551 - acc: 0.75 - ETA: 21s - loss: 0.5536 - acc: 0.75 - ETA: 19s - loss: 0.5523 - acc: 0.75 - ETA: 17s - loss: 0.5518 - acc: 0.75 - ETA: 15s - loss: 0.5496 - acc: 0.75 - ETA: 14s - loss: 0.5476 - acc: 0.75 - ETA: 12s - loss: 0.5471 - acc: 0.75 - ETA: 10s - loss: 0.5471 - acc: 0.75 - ETA: 8s - loss: 0.5439 - acc: 0.7573 - ETA: 6s - loss: 0.5419 - acc: 0.758 - ETA: 5s - loss: 0.5398 - acc: 0.759 - ETA: 3s - loss: 0.5411 - acc: 0.758 - ETA: 1s - loss: 0.5403 - acc: 0.757 - 104s 15ms/step - loss: 0.5392 - acc: 0.7585 - val_loss: 0.4675 - val_acc: 0.7862\n",
      "Epoch 2/10\n",
      "6773/6773 [==============================] - ETA: 1:16 - loss: 0.4905 - acc: 0.773 - ETA: 1:15 - loss: 0.4284 - acc: 0.820 - ETA: 1:14 - loss: 0.4212 - acc: 0.815 - ETA: 1:12 - loss: 0.4190 - acc: 0.824 - ETA: 1:10 - loss: 0.4253 - acc: 0.820 - ETA: 1:10 - loss: 0.4338 - acc: 0.809 - ETA: 1:12 - loss: 0.4410 - acc: 0.809 - ETA: 1:12 - loss: 0.4430 - acc: 0.805 - ETA: 1:09 - loss: 0.4327 - acc: 0.812 - ETA: 1:09 - loss: 0.4426 - acc: 0.805 - ETA: 1:07 - loss: 0.4442 - acc: 0.802 - ETA: 1:06 - loss: 0.4477 - acc: 0.802 - ETA: 1:04 - loss: 0.4478 - acc: 0.801 - ETA: 1:03 - loss: 0.4497 - acc: 0.799 - ETA: 1:01 - loss: 0.4479 - acc: 0.801 - ETA: 59s - loss: 0.4513 - acc: 0.798 - ETA: 57s - loss: 0.4489 - acc: 0.80 - ETA: 56s - loss: 0.4451 - acc: 0.80 - ETA: 54s - loss: 0.4453 - acc: 0.80 - ETA: 52s - loss: 0.4426 - acc: 0.80 - ETA: 50s - loss: 0.4402 - acc: 0.80 - ETA: 49s - loss: 0.4402 - acc: 0.80 - ETA: 47s - loss: 0.4382 - acc: 0.80 - ETA: 45s - loss: 0.4382 - acc: 0.80 - ETA: 44s - loss: 0.4430 - acc: 0.80 - ETA: 42s - loss: 0.4433 - acc: 0.80 - ETA: 40s - loss: 0.4433 - acc: 0.80 - ETA: 39s - loss: 0.4434 - acc: 0.80 - ETA: 37s - loss: 0.4419 - acc: 0.80 - ETA: 35s - loss: 0.4395 - acc: 0.81 - ETA: 34s - loss: 0.4395 - acc: 0.81 - ETA: 32s - loss: 0.4401 - acc: 0.80 - ETA: 31s - loss: 0.4354 - acc: 0.81 - ETA: 29s - loss: 0.4372 - acc: 0.81 - ETA: 27s - loss: 0.4382 - acc: 0.81 - ETA: 26s - loss: 0.4401 - acc: 0.80 - ETA: 24s - loss: 0.4400 - acc: 0.80 - ETA: 23s - loss: 0.4397 - acc: 0.81 - ETA: 21s - loss: 0.4387 - acc: 0.81 - ETA: 20s - loss: 0.4389 - acc: 0.81 - ETA: 18s - loss: 0.4381 - acc: 0.81 - ETA: 16s - loss: 0.4382 - acc: 0.81 - ETA: 15s - loss: 0.4403 - acc: 0.80 - ETA: 13s - loss: 0.4377 - acc: 0.81 - ETA: 12s - loss: 0.4367 - acc: 0.81 - ETA: 10s - loss: 0.4381 - acc: 0.80 - ETA: 9s - loss: 0.4371 - acc: 0.8088 - ETA: 7s - loss: 0.4383 - acc: 0.808 - ETA: 6s - loss: 0.4392 - acc: 0.808 - ETA: 4s - loss: 0.4399 - acc: 0.808 - ETA: 2s - loss: 0.4388 - acc: 0.809 - ETA: 1s - loss: 0.4390 - acc: 0.808 - 91s 13ms/step - loss: 0.4407 - acc: 0.8076 - val_loss: 0.4360 - val_acc: 0.8045\n",
      "Epoch 3/10\n",
      "6773/6773 [==============================] - ETA: 1:21 - loss: 0.5512 - acc: 0.750 - ETA: 1:18 - loss: 0.4321 - acc: 0.808 - ETA: 1:16 - loss: 0.4109 - acc: 0.815 - ETA: 1:15 - loss: 0.4132 - acc: 0.816 - ETA: 1:13 - loss: 0.4127 - acc: 0.814 - ETA: 1:13 - loss: 0.4069 - acc: 0.821 - ETA: 1:11 - loss: 0.4046 - acc: 0.821 - ETA: 1:09 - loss: 0.4083 - acc: 0.817 - ETA: 1:08 - loss: 0.4035 - acc: 0.820 - ETA: 1:06 - loss: 0.3999 - acc: 0.825 - ETA: 1:05 - loss: 0.4052 - acc: 0.821 - ETA: 1:04 - loss: 0.4016 - acc: 0.822 - ETA: 1:04 - loss: 0.4026 - acc: 0.822 - ETA: 1:03 - loss: 0.4109 - acc: 0.818 - ETA: 1:02 - loss: 0.4117 - acc: 0.815 - ETA: 1:00 - loss: 0.4074 - acc: 0.819 - ETA: 58s - loss: 0.4043 - acc: 0.821 - ETA: 56s - loss: 0.4001 - acc: 0.82 - ETA: 55s - loss: 0.3993 - acc: 0.82 - ETA: 53s - loss: 0.3963 - acc: 0.82 - ETA: 51s - loss: 0.3940 - acc: 0.82 - ETA: 50s - loss: 0.3891 - acc: 0.82 - ETA: 48s - loss: 0.3881 - acc: 0.82 - ETA: 46s - loss: 0.3921 - acc: 0.82 - ETA: 44s - loss: 0.3939 - acc: 0.82 - ETA: 43s - loss: 0.3999 - acc: 0.82 - ETA: 41s - loss: 0.3995 - acc: 0.82 - ETA: 39s - loss: 0.4019 - acc: 0.82 - ETA: 38s - loss: 0.4043 - acc: 0.82 - ETA: 36s - loss: 0.4050 - acc: 0.81 - ETA: 34s - loss: 0.4049 - acc: 0.82 - ETA: 33s - loss: 0.4050 - acc: 0.82 - ETA: 31s - loss: 0.4028 - acc: 0.82 - ETA: 30s - loss: 0.4048 - acc: 0.82 - ETA: 28s - loss: 0.4043 - acc: 0.82 - ETA: 26s - loss: 0.4044 - acc: 0.82 - ETA: 25s - loss: 0.4059 - acc: 0.82 - ETA: 23s - loss: 0.4051 - acc: 0.82 - ETA: 22s - loss: 0.4058 - acc: 0.82 - ETA: 20s - loss: 0.4052 - acc: 0.82 - ETA: 18s - loss: 0.4060 - acc: 0.82 - ETA: 17s - loss: 0.4047 - acc: 0.82 - ETA: 15s - loss: 0.4054 - acc: 0.82 - ETA: 14s - loss: 0.4051 - acc: 0.82 - ETA: 12s - loss: 0.4047 - acc: 0.82 - ETA: 10s - loss: 0.4052 - acc: 0.82 - ETA: 9s - loss: 0.4049 - acc: 0.8235 - ETA: 7s - loss: 0.4051 - acc: 0.822 - ETA: 6s - loss: 0.4056 - acc: 0.822 - ETA: 4s - loss: 0.4047 - acc: 0.822 - ETA: 3s - loss: 0.4060 - acc: 0.822 - ETA: 1s - loss: 0.4054 - acc: 0.822 - 92s 14ms/step - loss: 0.4054 - acc: 0.8227 - val_loss: 0.3969 - val_acc: 0.8263\n",
      "Epoch 4/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6773/6773 [==============================] - ETA: 1:18 - loss: 0.3337 - acc: 0.851 - ETA: 1:16 - loss: 0.3834 - acc: 0.820 - ETA: 1:14 - loss: 0.3625 - acc: 0.835 - ETA: 1:13 - loss: 0.3627 - acc: 0.841 - ETA: 1:11 - loss: 0.3810 - acc: 0.834 - ETA: 1:10 - loss: 0.3823 - acc: 0.838 - ETA: 1:08 - loss: 0.3803 - acc: 0.841 - ETA: 1:07 - loss: 0.3790 - acc: 0.837 - ETA: 1:05 - loss: 0.3749 - acc: 0.841 - ETA: 1:04 - loss: 0.3707 - acc: 0.842 - ETA: 1:02 - loss: 0.3817 - acc: 0.835 - ETA: 1:01 - loss: 0.3780 - acc: 0.839 - ETA: 59s - loss: 0.3697 - acc: 0.843 - ETA: 58s - loss: 0.3721 - acc: 0.84 - ETA: 56s - loss: 0.3749 - acc: 0.84 - ETA: 55s - loss: 0.3756 - acc: 0.84 - ETA: 54s - loss: 0.3777 - acc: 0.84 - ETA: 52s - loss: 0.3788 - acc: 0.84 - ETA: 51s - loss: 0.3806 - acc: 0.83 - ETA: 49s - loss: 0.3788 - acc: 0.84 - ETA: 47s - loss: 0.3840 - acc: 0.83 - ETA: 46s - loss: 0.3830 - acc: 0.83 - ETA: 44s - loss: 0.3820 - acc: 0.83 - ETA: 43s - loss: 0.3789 - acc: 0.83 - ETA: 41s - loss: 0.3792 - acc: 0.83 - ETA: 40s - loss: 0.3821 - acc: 0.83 - ETA: 38s - loss: 0.3842 - acc: 0.83 - ETA: 37s - loss: 0.3819 - acc: 0.83 - ETA: 35s - loss: 0.3825 - acc: 0.83 - ETA: 34s - loss: 0.3823 - acc: 0.83 - ETA: 32s - loss: 0.3816 - acc: 0.83 - ETA: 31s - loss: 0.3802 - acc: 0.83 - ETA: 29s - loss: 0.3799 - acc: 0.83 - ETA: 28s - loss: 0.3789 - acc: 0.84 - ETA: 26s - loss: 0.3759 - acc: 0.84 - ETA: 25s - loss: 0.3765 - acc: 0.84 - ETA: 23s - loss: 0.3772 - acc: 0.84 - ETA: 22s - loss: 0.3744 - acc: 0.84 - ETA: 20s - loss: 0.3750 - acc: 0.84 - ETA: 19s - loss: 0.3739 - acc: 0.84 - ETA: 17s - loss: 0.3741 - acc: 0.84 - ETA: 16s - loss: 0.3730 - acc: 0.84 - ETA: 14s - loss: 0.3745 - acc: 0.84 - ETA: 13s - loss: 0.3738 - acc: 0.84 - ETA: 11s - loss: 0.3746 - acc: 0.84 - ETA: 10s - loss: 0.3737 - acc: 0.84 - ETA: 8s - loss: 0.3745 - acc: 0.8424 - ETA: 7s - loss: 0.3758 - acc: 0.841 - ETA: 5s - loss: 0.3738 - acc: 0.842 - ETA: 4s - loss: 0.3735 - acc: 0.842 - ETA: 2s - loss: 0.3739 - acc: 0.841 - ETA: 1s - loss: 0.3740 - acc: 0.841 - 88s 13ms/step - loss: 0.3742 - acc: 0.8408 - val_loss: 0.3795 - val_acc: 0.8299\n",
      "Epoch 5/10\n",
      "6773/6773 [==============================] - ETA: 1:20 - loss: 0.4180 - acc: 0.828 - ETA: 1:18 - loss: 0.3977 - acc: 0.832 - ETA: 1:16 - loss: 0.3950 - acc: 0.828 - ETA: 1:14 - loss: 0.3999 - acc: 0.822 - ETA: 1:12 - loss: 0.3908 - acc: 0.825 - ETA: 1:11 - loss: 0.3860 - acc: 0.829 - ETA: 1:09 - loss: 0.3748 - acc: 0.838 - ETA: 1:07 - loss: 0.3694 - acc: 0.841 - ETA: 1:06 - loss: 0.3762 - acc: 0.837 - ETA: 1:04 - loss: 0.3692 - acc: 0.842 - ETA: 1:03 - loss: 0.3689 - acc: 0.843 - ETA: 1:01 - loss: 0.3592 - acc: 0.847 - ETA: 1:00 - loss: 0.3585 - acc: 0.845 - ETA: 58s - loss: 0.3604 - acc: 0.845 - ETA: 56s - loss: 0.3623 - acc: 0.84 - ETA: 55s - loss: 0.3584 - acc: 0.84 - ETA: 53s - loss: 0.3601 - acc: 0.84 - ETA: 52s - loss: 0.3600 - acc: 0.84 - ETA: 50s - loss: 0.3621 - acc: 0.84 - ETA: 49s - loss: 0.3653 - acc: 0.84 - ETA: 47s - loss: 0.3623 - acc: 0.84 - ETA: 46s - loss: 0.3626 - acc: 0.84 - ETA: 44s - loss: 0.3629 - acc: 0.84 - ETA: 43s - loss: 0.3643 - acc: 0.84 - ETA: 41s - loss: 0.3661 - acc: 0.84 - ETA: 40s - loss: 0.3660 - acc: 0.84 - ETA: 38s - loss: 0.3661 - acc: 0.84 - ETA: 37s - loss: 0.3675 - acc: 0.83 - ETA: 35s - loss: 0.3663 - acc: 0.84 - ETA: 34s - loss: 0.3662 - acc: 0.84 - ETA: 32s - loss: 0.3675 - acc: 0.84 - ETA: 31s - loss: 0.3651 - acc: 0.84 - ETA: 29s - loss: 0.3654 - acc: 0.84 - ETA: 28s - loss: 0.3671 - acc: 0.84 - ETA: 26s - loss: 0.3654 - acc: 0.84 - ETA: 25s - loss: 0.3658 - acc: 0.84 - ETA: 23s - loss: 0.3650 - acc: 0.84 - ETA: 22s - loss: 0.3656 - acc: 0.84 - ETA: 20s - loss: 0.3690 - acc: 0.84 - ETA: 19s - loss: 0.3698 - acc: 0.84 - ETA: 17s - loss: 0.3695 - acc: 0.84 - ETA: 16s - loss: 0.3691 - acc: 0.84 - ETA: 14s - loss: 0.3682 - acc: 0.84 - ETA: 13s - loss: 0.3681 - acc: 0.84 - ETA: 11s - loss: 0.3689 - acc: 0.84 - ETA: 10s - loss: 0.3673 - acc: 0.84 - ETA: 8s - loss: 0.3652 - acc: 0.8454 - ETA: 7s - loss: 0.3704 - acc: 0.843 - ETA: 5s - loss: 0.3694 - acc: 0.843 - ETA: 4s - loss: 0.3687 - acc: 0.844 - ETA: 2s - loss: 0.3674 - acc: 0.845 - ETA: 1s - loss: 0.3659 - acc: 0.846 - 88s 13ms/step - loss: 0.3652 - acc: 0.8469 - val_loss: 0.3643 - val_acc: 0.8328\n",
      "Epoch 6/10\n",
      "6773/6773 [==============================] - ETA: 1:17 - loss: 0.3943 - acc: 0.828 - ETA: 1:16 - loss: 0.3536 - acc: 0.863 - ETA: 1:14 - loss: 0.3227 - acc: 0.880 - ETA: 1:12 - loss: 0.3480 - acc: 0.857 - ETA: 1:11 - loss: 0.3660 - acc: 0.851 - ETA: 1:10 - loss: 0.3648 - acc: 0.852 - ETA: 1:08 - loss: 0.3729 - acc: 0.847 - ETA: 1:07 - loss: 0.3648 - acc: 0.850 - ETA: 1:05 - loss: 0.3671 - acc: 0.849 - ETA: 1:03 - loss: 0.3668 - acc: 0.850 - ETA: 1:02 - loss: 0.3673 - acc: 0.853 - ETA: 1:01 - loss: 0.3695 - acc: 0.850 - ETA: 59s - loss: 0.3649 - acc: 0.854 - ETA: 58s - loss: 0.3655 - acc: 0.85 - ETA: 56s - loss: 0.3693 - acc: 0.84 - ETA: 55s - loss: 0.3658 - acc: 0.84 - ETA: 53s - loss: 0.3619 - acc: 0.85 - ETA: 52s - loss: 0.3627 - acc: 0.85 - ETA: 53s - loss: 0.3662 - acc: 0.85 - ETA: 53s - loss: 0.3651 - acc: 0.85 - ETA: 51s - loss: 0.3624 - acc: 0.85 - ETA: 50s - loss: 0.3629 - acc: 0.85 - ETA: 48s - loss: 0.3637 - acc: 0.85 - ETA: 46s - loss: 0.3640 - acc: 0.85 - ETA: 44s - loss: 0.3607 - acc: 0.85 - ETA: 43s - loss: 0.3586 - acc: 0.85 - ETA: 41s - loss: 0.3587 - acc: 0.85 - ETA: 39s - loss: 0.3591 - acc: 0.85 - ETA: 38s - loss: 0.3559 - acc: 0.85 - ETA: 36s - loss: 0.3558 - acc: 0.85 - ETA: 34s - loss: 0.3540 - acc: 0.85 - ETA: 33s - loss: 0.3510 - acc: 0.85 - ETA: 31s - loss: 0.3526 - acc: 0.85 - ETA: 29s - loss: 0.3532 - acc: 0.85 - ETA: 28s - loss: 0.3512 - acc: 0.85 - ETA: 26s - loss: 0.3507 - acc: 0.85 - ETA: 25s - loss: 0.3508 - acc: 0.85 - ETA: 23s - loss: 0.3521 - acc: 0.85 - ETA: 21s - loss: 0.3520 - acc: 0.85 - ETA: 20s - loss: 0.3505 - acc: 0.85 - ETA: 18s - loss: 0.3497 - acc: 0.85 - ETA: 17s - loss: 0.3494 - acc: 0.85 - ETA: 15s - loss: 0.3478 - acc: 0.85 - ETA: 13s - loss: 0.3465 - acc: 0.85 - ETA: 12s - loss: 0.3466 - acc: 0.85 - ETA: 10s - loss: 0.3458 - acc: 0.85 - ETA: 9s - loss: 0.3452 - acc: 0.8569 - ETA: 7s - loss: 0.3465 - acc: 0.856 - ETA: 6s - loss: 0.3461 - acc: 0.856 - ETA: 4s - loss: 0.3459 - acc: 0.856 - ETA: 2s - loss: 0.3462 - acc: 0.856 - ETA: 1s - loss: 0.3464 - acc: 0.856 - 91s 13ms/step - loss: 0.3461 - acc: 0.8565 - val_loss: 0.3533 - val_acc: 0.8423\n",
      "Epoch 7/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6773/6773 [==============================] - ETA: 1:17 - loss: 0.2611 - acc: 0.898 - ETA: 1:18 - loss: 0.2639 - acc: 0.894 - ETA: 1:16 - loss: 0.2809 - acc: 0.885 - ETA: 1:14 - loss: 0.2844 - acc: 0.882 - ETA: 1:12 - loss: 0.2939 - acc: 0.875 - ETA: 1:11 - loss: 0.3025 - acc: 0.869 - ETA: 1:09 - loss: 0.3048 - acc: 0.870 - ETA: 1:08 - loss: 0.3088 - acc: 0.868 - ETA: 1:06 - loss: 0.3081 - acc: 0.871 - ETA: 1:05 - loss: 0.3179 - acc: 0.870 - ETA: 1:03 - loss: 0.3165 - acc: 0.870 - ETA: 1:02 - loss: 0.3184 - acc: 0.867 - ETA: 1:00 - loss: 0.3143 - acc: 0.869 - ETA: 59s - loss: 0.3124 - acc: 0.870 - ETA: 57s - loss: 0.3151 - acc: 0.86 - ETA: 56s - loss: 0.3115 - acc: 0.87 - ETA: 54s - loss: 0.3123 - acc: 0.87 - ETA: 52s - loss: 0.3110 - acc: 0.87 - ETA: 51s - loss: 0.3127 - acc: 0.87 - ETA: 50s - loss: 0.3171 - acc: 0.87 - ETA: 50s - loss: 0.3196 - acc: 0.87 - ETA: 49s - loss: 0.3201 - acc: 0.87 - ETA: 47s - loss: 0.3198 - acc: 0.86 - ETA: 45s - loss: 0.3200 - acc: 0.86 - ETA: 44s - loss: 0.3223 - acc: 0.86 - ETA: 42s - loss: 0.3230 - acc: 0.86 - ETA: 41s - loss: 0.3240 - acc: 0.86 - ETA: 39s - loss: 0.3255 - acc: 0.86 - ETA: 37s - loss: 0.3264 - acc: 0.86 - ETA: 36s - loss: 0.3246 - acc: 0.86 - ETA: 34s - loss: 0.3249 - acc: 0.86 - ETA: 32s - loss: 0.3244 - acc: 0.86 - ETA: 31s - loss: 0.3244 - acc: 0.86 - ETA: 29s - loss: 0.3249 - acc: 0.86 - ETA: 28s - loss: 0.3253 - acc: 0.86 - ETA: 26s - loss: 0.3278 - acc: 0.86 - ETA: 24s - loss: 0.3283 - acc: 0.86 - ETA: 23s - loss: 0.3303 - acc: 0.86 - ETA: 22s - loss: 0.3311 - acc: 0.86 - ETA: 1:07 - loss: 0.3327 - acc: 0.862 - ETA: 1:00 - loss: 0.3332 - acc: 0.861 - ETA: 54s - loss: 0.3359 - acc: 0.859 - ETA: 48s - loss: 0.3339 - acc: 0.86 - ETA: 43s - loss: 0.3345 - acc: 0.86 - ETA: 37s - loss: 0.3331 - acc: 0.86 - ETA: 32s - loss: 0.3326 - acc: 0.86 - ETA: 27s - loss: 0.3325 - acc: 0.86 - ETA: 22s - loss: 0.3319 - acc: 0.86 - ETA: 17s - loss: 0.3301 - acc: 0.86 - ETA: 13s - loss: 0.3307 - acc: 0.86 - ETA: 8s - loss: 0.3316 - acc: 0.8615 - ETA: 3s - loss: 0.3303 - acc: 0.862 - 236s 35ms/step - loss: 0.3304 - acc: 0.8624 - val_loss: 0.3520 - val_acc: 0.8482\n",
      "Epoch 8/10\n",
      "6773/6773 [==============================] - ETA: 1:17 - loss: 0.2961 - acc: 0.906 - ETA: 1:16 - loss: 0.2859 - acc: 0.894 - ETA: 1:16 - loss: 0.3207 - acc: 0.867 - ETA: 1:14 - loss: 0.3391 - acc: 0.851 - ETA: 1:12 - loss: 0.3505 - acc: 0.850 - ETA: 1:11 - loss: 0.3540 - acc: 0.850 - ETA: 1:09 - loss: 0.3404 - acc: 0.859 - ETA: 1:07 - loss: 0.3404 - acc: 0.857 - ETA: 1:06 - loss: 0.3364 - acc: 0.859 - ETA: 1:04 - loss: 0.3390 - acc: 0.860 - ETA: 1:03 - loss: 0.3382 - acc: 0.862 - ETA: 1:01 - loss: 0.3358 - acc: 0.863 - ETA: 1:00 - loss: 0.3304 - acc: 0.866 - ETA: 58s - loss: 0.3316 - acc: 0.864 - ETA: 57s - loss: 0.3246 - acc: 0.86 - ETA: 55s - loss: 0.3203 - acc: 0.86 - ETA: 54s - loss: 0.3184 - acc: 0.86 - ETA: 53s - loss: 0.3210 - acc: 0.86 - ETA: 52s - loss: 0.3204 - acc: 0.86 - ETA: 50s - loss: 0.3181 - acc: 0.86 - ETA: 49s - loss: 0.3164 - acc: 0.86 - ETA: 47s - loss: 0.3167 - acc: 0.87 - ETA: 46s - loss: 0.3167 - acc: 0.87 - ETA: 44s - loss: 0.3184 - acc: 0.87 - ETA: 43s - loss: 0.3185 - acc: 0.87 - ETA: 41s - loss: 0.3179 - acc: 0.87 - ETA: 39s - loss: 0.3183 - acc: 0.87 - ETA: 38s - loss: 0.3179 - acc: 0.87 - ETA: 36s - loss: 0.3169 - acc: 0.86 - ETA: 35s - loss: 0.3164 - acc: 0.86 - ETA: 33s - loss: 0.3211 - acc: 0.86 - ETA: 32s - loss: 0.3201 - acc: 0.86 - ETA: 30s - loss: 0.3187 - acc: 0.86 - ETA: 28s - loss: 0.3188 - acc: 0.86 - ETA: 27s - loss: 0.3195 - acc: 0.86 - ETA: 25s - loss: 0.3190 - acc: 0.87 - ETA: 24s - loss: 0.3180 - acc: 0.87 - ETA: 22s - loss: 0.3172 - acc: 0.87 - ETA: 21s - loss: 0.3185 - acc: 0.87 - ETA: 19s - loss: 0.3205 - acc: 0.86 - ETA: 18s - loss: 0.3233 - acc: 0.86 - ETA: 16s - loss: 0.3226 - acc: 0.86 - ETA: 15s - loss: 0.3213 - acc: 0.86 - ETA: 13s - loss: 0.3201 - acc: 0.86 - ETA: 12s - loss: 0.3194 - acc: 0.86 - ETA: 10s - loss: 0.3193 - acc: 0.86 - ETA: 9s - loss: 0.3188 - acc: 0.8705 - ETA: 7s - loss: 0.3187 - acc: 0.870 - ETA: 5s - loss: 0.3202 - acc: 0.870 - ETA: 4s - loss: 0.3199 - acc: 0.870 - ETA: 2s - loss: 0.3190 - acc: 0.870 - ETA: 1s - loss: 0.3190 - acc: 0.870 - 89s 13ms/step - loss: 0.3181 - acc: 0.8708 - val_loss: 0.3371 - val_acc: 0.8523\n",
      "Epoch 9/10\n",
      "6773/6773 [==============================] - ETA: 1:18 - loss: 0.3001 - acc: 0.867 - ETA: 1:16 - loss: 0.2862 - acc: 0.867 - ETA: 1:16 - loss: 0.2735 - acc: 0.872 - ETA: 1:14 - loss: 0.2907 - acc: 0.867 - ETA: 1:13 - loss: 0.2884 - acc: 0.867 - ETA: 1:15 - loss: 0.2821 - acc: 0.876 - ETA: 1:13 - loss: 0.2888 - acc: 0.875 - ETA: 1:12 - loss: 0.3026 - acc: 0.867 - ETA: 1:10 - loss: 0.2941 - acc: 0.874 - ETA: 1:08 - loss: 0.3017 - acc: 0.874 - ETA: 1:06 - loss: 0.2991 - acc: 0.876 - ETA: 1:05 - loss: 0.3006 - acc: 0.876 - ETA: 1:04 - loss: 0.3029 - acc: 0.875 - ETA: 1:03 - loss: 0.3023 - acc: 0.875 - ETA: 1:01 - loss: 0.3017 - acc: 0.875 - ETA: 59s - loss: 0.2978 - acc: 0.876 - ETA: 57s - loss: 0.2991 - acc: 0.87 - ETA: 55s - loss: 0.2967 - acc: 0.87 - ETA: 53s - loss: 0.2986 - acc: 0.87 - ETA: 52s - loss: 0.2969 - acc: 0.87 - ETA: 51s - loss: 0.2944 - acc: 0.87 - ETA: 51s - loss: 0.2952 - acc: 0.87 - ETA: 51s - loss: 0.2982 - acc: 0.87 - ETA: 50s - loss: 0.2951 - acc: 0.87 - ETA: 49s - loss: 0.2944 - acc: 0.87 - ETA: 47s - loss: 0.2954 - acc: 0.87 - ETA: 46s - loss: 0.2962 - acc: 0.87 - ETA: 44s - loss: 0.2990 - acc: 0.87 - ETA: 43s - loss: 0.2977 - acc: 0.87 - ETA: 41s - loss: 0.2975 - acc: 0.87 - ETA: 39s - loss: 0.2972 - acc: 0.87 - ETA: 38s - loss: 0.3009 - acc: 0.87 - ETA: 36s - loss: 0.3001 - acc: 0.87 - ETA: 34s - loss: 0.3026 - acc: 0.87 - ETA: 32s - loss: 0.3041 - acc: 0.87 - ETA: 31s - loss: 0.3053 - acc: 0.87 - ETA: 29s - loss: 0.3061 - acc: 0.87 - ETA: 27s - loss: 0.3051 - acc: 0.87 - ETA: 26s - loss: 0.3044 - acc: 0.87 - ETA: 24s - loss: 0.3043 - acc: 0.87 - ETA: 23s - loss: 0.3026 - acc: 0.87 - ETA: 21s - loss: 0.3020 - acc: 0.87 - ETA: 19s - loss: 0.3019 - acc: 0.87 - ETA: 17s - loss: 0.3019 - acc: 0.87 - ETA: 15s - loss: 0.3011 - acc: 0.87 - ETA: 13s - loss: 0.2999 - acc: 0.87 - ETA: 11s - loss: 0.3002 - acc: 0.87 - ETA: 9s - loss: 0.3000 - acc: 0.8743 - ETA: 7s - loss: 0.3022 - acc: 0.873 - ETA: 5s - loss: 0.3020 - acc: 0.873 - ETA: 3s - loss: 0.3008 - acc: 0.874 - ETA: 1s - loss: 0.3014 - acc: 0.874 - 113s 17ms/step - loss: 0.3022 - acc: 0.8732 - val_loss: 0.3329 - val_acc: 0.8571\n",
      "Epoch 10/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6773/6773 [==============================] - ETA: 1:32 - loss: 0.3123 - acc: 0.859 - ETA: 1:26 - loss: 0.3208 - acc: 0.859 - ETA: 1:38 - loss: 0.3172 - acc: 0.867 - ETA: 1:48 - loss: 0.3069 - acc: 0.877 - ETA: 1:48 - loss: 0.3103 - acc: 0.867 - ETA: 1:49 - loss: 0.3044 - acc: 0.875 - ETA: 1:48 - loss: 0.2972 - acc: 0.877 - ETA: 1:46 - loss: 0.2996 - acc: 0.875 - ETA: 1:42 - loss: 0.2989 - acc: 0.869 - ETA: 1:38 - loss: 0.3052 - acc: 0.868 - ETA: 1:34 - loss: 0.3064 - acc: 0.866 - ETA: 1:31 - loss: 0.3037 - acc: 0.867 - ETA: 1:28 - loss: 0.3073 - acc: 0.865 - ETA: 1:25 - loss: 0.3107 - acc: 0.864 - ETA: 1:22 - loss: 0.3056 - acc: 0.868 - ETA: 1:20 - loss: 0.2990 - acc: 0.873 - ETA: 1:17 - loss: 0.2949 - acc: 0.876 - ETA: 1:15 - loss: 0.2919 - acc: 0.876 - ETA: 1:13 - loss: 0.2921 - acc: 0.877 - ETA: 1:10 - loss: 0.2953 - acc: 0.876 - ETA: 1:08 - loss: 0.2939 - acc: 0.877 - ETA: 1:05 - loss: 0.2940 - acc: 0.877 - ETA: 1:03 - loss: 0.2934 - acc: 0.877 - ETA: 1:01 - loss: 0.2931 - acc: 0.877 - ETA: 59s - loss: 0.2922 - acc: 0.877 - ETA: 56s - loss: 0.2919 - acc: 0.87 - ETA: 54s - loss: 0.2911 - acc: 0.87 - ETA: 52s - loss: 0.2902 - acc: 0.88 - ETA: 50s - loss: 0.2905 - acc: 0.87 - ETA: 48s - loss: 0.2910 - acc: 0.87 - ETA: 45s - loss: 0.2910 - acc: 0.88 - ETA: 44s - loss: 0.2917 - acc: 0.87 - ETA: 42s - loss: 0.2923 - acc: 0.87 - ETA: 41s - loss: 0.2915 - acc: 0.87 - ETA: 40s - loss: 0.2906 - acc: 0.87 - ETA: 37s - loss: 0.2906 - acc: 0.87 - ETA: 35s - loss: 0.2925 - acc: 0.87 - ETA: 33s - loss: 0.2911 - acc: 0.87 - ETA: 30s - loss: 0.2917 - acc: 0.87 - ETA: 28s - loss: 0.2916 - acc: 0.87 - ETA: 26s - loss: 0.2924 - acc: 0.87 - ETA: 24s - loss: 0.2933 - acc: 0.87 - ETA: 21s - loss: 0.2956 - acc: 0.87 - ETA: 19s - loss: 0.2946 - acc: 0.87 - ETA: 17s - loss: 0.2933 - acc: 0.87 - ETA: 15s - loss: 0.2914 - acc: 0.87 - ETA: 12s - loss: 0.2918 - acc: 0.87 - ETA: 10s - loss: 0.2914 - acc: 0.87 - ETA: 8s - loss: 0.2911 - acc: 0.8766 - ETA: 6s - loss: 0.2895 - acc: 0.877 - ETA: 4s - loss: 0.2890 - acc: 0.877 - ETA: 1s - loss: 0.2898 - acc: 0.876 - 124s 18ms/step - loss: 0.2902 - acc: 0.8760 - val_loss: 0.3307 - val_acc: 0.8606\n"
     ]
    }
   ],
   "source": [
    "model_1 = Model_GRU_CNN(embedding_dim = 100, model = glove_model, lr = 1e-3, lr_d = 0, units = 112, dr = 0.2)\n",
    "best_model_path = model_1.train_model()\n",
    "# from keras.models import load_model\n",
    "# model_1 = load_model(r'C:\\Users\\admin\\InternshipTask\\NLP Task\\checkpoints\\1553572267\\lstm_100_0_0.00_0.20.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(model_1.history.history['loss'])\n",
    "plt.plot(model_1.history.history['val_loss'])\n",
    "plt.title('Model Loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_test_set(df, model, max_seq_len):\n",
    "    \n",
    "    total_array = list(df['sentence'])\n",
    "    \n",
    "    total_array1 = []\n",
    "    for sent in total_array:\n",
    "        arrayin=[]\n",
    "        for word in sent.split(' '):\n",
    "            word = word.replace(' ','')\n",
    "            try:\n",
    "                a = model[word]\n",
    "                arrayin.append(a)\n",
    "            except Exception as e:\n",
    "                #print(e) -- for out of dictionary words\n",
    "                continue\n",
    "        total_array1.append(arrayin)\n",
    "    \n",
    "    total_array = total_array1\n",
    "    \n",
    "    # Padding the string sequences\n",
    "    test_padded_data = pad_sequences(total_array, maxlen=max_seq_len)\n",
    "    \n",
    "    test_data=test_padded_data\n",
    "    test_labels = df['label']\n",
    "    return test_data, test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data, test_label = create_test_set(val_df, glove_model, model_1.max_len)\n",
    "model = load_model(best_model_path)\n",
    "y_preds = model.predict(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6569646569646569\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          NS       0.66      0.91      0.77       296\n",
      "           S       0.85      0.53      0.66       296\n",
      "\n",
      "   micro avg       0.72      0.72      0.72       592\n",
      "   macro avg       0.76      0.72      0.71       592\n",
      "weighted avg       0.76      0.72      0.71       592\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = (y_preds >= 0.5).astype(int)\n",
    "y_real = test_label\n",
    "print(f1_score(y_real, y_pred))\n",
    "print(classification_report(y_real, y_pred, target_names=['NS','S']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running model on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data, _ = create_test_set(test_df, glove_model, model_1.max_len)\n",
    "nn_y_preds = model.predict(test_data)\n",
    "nn_y_pred = (nn_y_preds >= 0.5).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) Applying fastText classification for a baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converting Training set to FastText Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sentence</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>663_3</td>\n",
       "      <td>__label__1 please enable removing language cod...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>663_4</td>\n",
       "      <td>__label__0 note in your csproj file there is a...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>664_1</td>\n",
       "      <td>__label__0 which means the new version not ful...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>664_2</td>\n",
       "      <td>__label__0 some of my users will still receive...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>664_3</td>\n",
       "      <td>__label__0 the store randomly gives the old xa...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id                                           sentence  label\n",
       "0  663_3  __label__1 please enable removing language cod...      1\n",
       "1  663_4  __label__0 note in your csproj file there is a...      0\n",
       "2  664_1  __label__0 which means the new version not ful...      0\n",
       "3  664_2  __label__0 some of my users will still receive...      0\n",
       "4  664_3  __label__0 the store randomly gives the old xa...      0"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "san = train_df['sentence'].copy()\n",
    "for i in range(len(san)):\n",
    "    san[i] = '__label__'+str(train_df['label'][i])+' '+san[i]\n",
    "train_df['sentence'] = san\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "fw = open('data/train.txt','w',encoding='utf8')\n",
    "for i in range(len(train_df)):\n",
    "    fw.write(train_df['sentence'][i])\n",
    "    fw.write('\\n')\n",
    "fw.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = ft.supervised('data/train.txt','data/model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fw = open('data/test.txt','w',encoding='utf8')\n",
    "# for i in range(len(test_df)):\n",
    "#     fw.write(test_df['sentence'][i])\n",
    "#     fw.write('\\n')\n",
    "# fw.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7023809523809524\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          NS       0.69      0.90      0.78       296\n",
      "           S       0.85      0.60      0.70       296\n",
      "\n",
      "   micro avg       0.75      0.75      0.75       592\n",
      "   macro avg       0.77      0.75      0.74       592\n",
      "weighted avg       0.77      0.75      0.74       592\n",
      "\n"
     ]
    }
   ],
   "source": [
    "val_texts = []\n",
    "for i in range(len(val_df)):\n",
    "    val_texts.append(val_df['sentence'][i])\n",
    "\n",
    "pred = clf.predict(val_texts)\n",
    "val_ft_pred = []\n",
    "for i in range(len(pred)):\n",
    "    val_ft_pred.append(int(pred[i][0]))\n",
    "val_labels = list(val_df['label'])\n",
    "print(f1_score([int(x) for x in val_labels], val_ft_pred))\n",
    "print(classification_report([int(x) for x in val_labels], val_ft_pred, target_names=['NS','S']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running model on Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_texts = []\n",
    "for i in range(len(test_df)):\n",
    "    test_texts.append(test_df['sentence'][i])\n",
    "\n",
    "pred = clf.predict(test_texts)\n",
    "test_ft_pred = []\n",
    "for i in range(len(pred)):\n",
    "    test_ft_pred.append(int(pred[i][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Selecting BERT FineTuned model for submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sentence</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9566</td>\n",
       "      <td>this would enable live traffic aware apps</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9569</td>\n",
       "      <td>please try other formatting like bold italics ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9576</td>\n",
       "      <td>since computers were invented to save time i s...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9577</td>\n",
       "      <td>allow rearranging if the user wants to change ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9579</td>\n",
       "      <td>add simd instructions for better use of arm ne...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     id                                           sentence  label\n",
       "0  9566          this would enable live traffic aware apps      0\n",
       "1  9569  please try other formatting like bold italics ...      1\n",
       "2  9576  since computers were invented to save time i s...      1\n",
       "3  9577  allow rearranging if the user wants to change ...      1\n",
       "4  9579  add simd instructions for better use of arm ne...      1"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub = pd.DataFrame()\n",
    "sub['id']=test_df['id']\n",
    "sub['sentence']=test_df['sentence']\n",
    "sub['label']=bert_test_preds['label_pred']\n",
    "sub.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub.to_csv('swapnil_parekh.csv', index=False, header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CREDITS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. https://github.com/google-research/bert\n",
    "2. https://github.com/hanxiao/bert-as-service\n",
    "3. GLOVE vectors"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TensorFlow-GPU",
   "language": "python",
   "name": "tf-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
